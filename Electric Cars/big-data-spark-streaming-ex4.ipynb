{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be2b64a-630f-435b-a9b3-0ae95837a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /Users/galrubinstein/.venv3.11/lib/python3.11/site-packages (2.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d7971e-1a64-4485-9372-6a9af2c7071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/galrubinstein/.venv3.11/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /Users/galrubinstein/.venv3.11/lib/python3.11/site-packages (from pyspark) (0.10.9.9)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a24539-1a85-443f-9ed6-23d7f49e32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b371880-0687-4de7-b835-7011036ab021",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_2.13:{pyspark.__version__} pyspark-shell'\n",
    "os.environ['SPARK_SUBMIT_OPTS'] = '-Djdk.security.auth.login.Config=ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179ddbcf-e897-4605-b7d0-935e00fb0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_URL = \"localhost:9092\"\n",
    "KAFKA_TOPIC = \"dev-rt_electric_cars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae29fcd-50f0-4ca1-82ea-ad1993c462f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, expr\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd5bfea-aaa6-4461-9fdc-34b95fecd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/11 15:09:06 WARN Utils: Your hostname, h-MacBook-Pro-sl-Gal.local, resolves to a loopback address: 127.0.0.1; using 192.168.167.55 instead (on interface en0)\n",
      "26/01/11 15:09:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/galrubinstein/.venv3.11/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/galrubinstein/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/galrubinstein/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ae5f4ab0-e1f8-403e-9fc5-3583a73a5666;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.1.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.1.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.8 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.17 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.1 in central\n",
      ":: resolution report :: resolve 214ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.1.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.17 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ae5f4ab0-e1f8-403e-9fc5-3583a73a5666\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/5ms)\n",
      "26/01/11 15:09:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-jupyter-streaming-electric-cars\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:{pyspark.__version__}\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"./checkpoint\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e1cfb7-0e13-4a81-9ade-d6ab5b4a56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER_URL) \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200dd6c4-5827-488c-aa49-9f5a5bea186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"VIN (1-10)\", StringType()) \\\n",
    "    .add(\"County\", StringType()) \\\n",
    "    .add(\"City\", StringType()) \\\n",
    "    .add(\"State\", StringType()) \\\n",
    "    .add(\"Model Year\", StringType()) \\\n",
    "    .add(\"Make\", StringType()) \\\n",
    "    .add(\"Model\", StringType())\n",
    "\n",
    "# Transform data to dataframe of json format\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Drop rows that contain ANY null values\n",
    "# This ensures that if a cell value is missing, the row is discarded\n",
    "clean_df = parsed_df.dropna()\n",
    "\n",
    "# Transform data to dataframe of json format and calculate the desired result\n",
    "result_df = clean_df.filter(col(\"Model Year\") == '2023') \\\n",
    "    .groupBy(\"City\", \"County\", \"State\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f015e7-8d92-4b8d-b394-1272c323642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 15:09:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "26/01/11 15:09:23 WARN MicroBatchExecution: Disabling AQE since AQE is not supported in stateful workloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x10bb19610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "result_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"res\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb961aa-8764-4832-b1e3-df0b1ed82394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+-----+\n",
      "|    City|County|State|count|\n",
      "+--------+------+-----+-----+\n",
      "| Seattle|  King|   WA| 5610|\n",
      "|Bellevue|  King|   WA| 2088|\n",
      "| Tukwila|  King|   WA| 1773|\n",
      "+--------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from res\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
